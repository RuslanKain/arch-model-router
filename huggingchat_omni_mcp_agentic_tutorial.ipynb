{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HuggingChat **Omni** + MCP: Multimodal Agentic Workflows (Tutorial)\n",
        "\n",
        "This follow‑up notebook shows how to build **2–3 agentic, multimodal workflows** that *route* a task to the best open model, and expose those capabilities through an **MCP (Model Context Protocol) server** so tools like Claude Desktop / Cursor / VS Code clients can call them.\n",
        "\n",
        "**Workflows included**\n",
        "1. **Code Generation + Reflection Review** (auto‑routing → draft → self‑critique → revision, optional quick tests)\n",
        "2. **Media Generation & Refinement** (image **and** audio flows with iterative feedback)\n",
        "3. **Web‑Scraping → Data Extraction → Structured Output** (CSV/JSON)\n",
        "\n",
        "> ℹ️ *Omni routing concept:* HuggingChat **Omni** picks a good open model for each request. Below we replicate that *idea* locally using a small router model (default: `katanemo/Arch-Router-1.5B`) plus simple policies, then call Hugging Face Inference API models per task. Switch models as you like.\n",
        "\n",
        "**What you’ll get**\n",
        "- A minimal **router** + **agents** for text / image / audio / web data\n",
        "- A runnable **MCP server** exposing tools: `infer_intent`, `code_generate`, `code_review`, `image_generate`, `audio_tts`, `asr_transcribe`, `web_search`, `web_fetch`, `extract_table`\n",
        "- End‑to‑end examples you can run as plain Python or via MCP clients\n",
        "\n",
        "---\n",
        "**Setup notes**\n",
        "- You’ll need a **Hugging Face token** with Inference API access: set `HF_TOKEN` as an env var.\n",
        "- Some cells call `pip install` for optional extras (beautifulsoup4, readability-lxml, duckduckgo-search, mcp, etc.).\n",
        "- The *router* cell can use an LLM router (`katanemo/Arch-Router-1.5B`) or fallback heuristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Install (optional) ---\n",
        "# If you already have these, you can skip.\n",
        "%%bash\n",
        "pip -q install --upgrade huggingface_hub httpx\n",
        "pip -q install beautifulsoup4 readability-lxml duckduckgo-search pandas matplotlib python-dotenv\n",
        "pip -q install \"mcp>=1.2.0\"  # Model Context Protocol server SDK\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Imports & basic config ---\n",
        "import os, io, json, time, base64, textwrap, pathlib, tempfile\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Dict, Any, List, Tuple\n",
        "import httpx\n",
        "from huggingface_hub import InferenceClient\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\", \"\")\n",
        "if not HF_TOKEN:\n",
        "    print(\"⚠️ Set HF_TOKEN in your environment to enable real API calls.\")\n",
        "\n",
        "# A simple helper to create an HF Inference client\n",
        "def hf_client(base_url: Optional[str] = None):\n",
        "    if base_url:\n",
        "        return InferenceClient(token=HF_TOKEN, base_url=base_url)\n",
        "    return InferenceClient(token=HF_TOKEN)\n",
        "\n",
        "# Save artifacts (images/audio/CSVs)\n",
        "ARTIFACTS_DIR = pathlib.Path(\"artifacts\")\n",
        "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
        "ARTIFACTS_DIR.resolve()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Router: \"Omni\"-style selection\n",
        "\n",
        "We’ll use a small router model by default (`katanemo/Arch-Router-1.5B`) with a simple system prompt that lists our available *routes*. If the router call fails or isn’t available, we fall back to **heuristics** based on keywords.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "ROUTER_MODEL = \"katanemo/Arch-Router-1.5B\"  # used by HuggingChat chat-ui as an optional router\n",
        "\n",
        "ROUTES = [\n",
        "    {\"name\": \"code_generation\", \"description\": \"Generate code, functions, scripts\"},\n",
        "    {\"name\": \"code_review\", \"description\": \"Reflect on code, find bugs, suggest fixes\"},\n",
        "    {\"name\": \"image_generation\", \"description\": \"Create or edit images from prompts\"},\n",
        "    {\"name\": \"audio_tts\", \"description\": \"Text-to-speech voice generation\"},\n",
        "    {\"name\": \"asr_transcribe\", \"description\": \"Speech-to-text transcription\"},\n",
        "    {\"name\": \"web_search\", \"description\": \"Find URLs for a topic\"},\n",
        "    {\"name\": \"web_fetch\", \"description\": \"Fetch + clean a page\"},\n",
        "    {\"name\": \"extract_table\", \"description\": \"Extract tabular data and save CSV\"},\n",
        "    {\"name\": \"general\", \"description\": \"General assistant / reasoning\"},\n",
        "]\n",
        "\n",
        "ROUTER_SYSTEM = f\"\"\"\n",
        "You are a routing model. Given the user's latest request and recent context, reply with JSON only: {{\\\"route\\\": \\\"<exact_route_name>\\\"}}.\n",
        "Choose from these routes:\n",
        "{json.dumps(ROUTES)}\n",
        "Rules:\n",
        "1) If request is about code creation, choose code_generation. If code critique or fix, choose code_review.\n",
        "2) If prompt asks to make an image, choose image_generation. If synthesize voice from text, audio_tts. If transcribe audio, asr_transcribe.\n",
        "3) If the user asks to find or browse information online, choose web_search; if they give a URL to retrieve/clean, choose web_fetch; if they ask to turn page content into a table/CSV, choose extract_table.\n",
        "4) Otherwise choose general.\n",
        "\"\"\"\n",
        "\n",
        "def route_with_model(query: str, history: Optional[List[Dict[str, str]]] = None) -> str:\n",
        "    \"\"\"Route using the Arch-Router model. Falls back to heuristics on failure.\"\"\"\n",
        "    history = history or []\n",
        "    try:\n",
        "        if not HF_TOKEN:\n",
        "            raise RuntimeError(\"HF token missing; using heuristic fallback.\")\n",
        "        client = hf_client()\n",
        "        messages = [{\"role\": \"system\", \"content\": ROUTER_SYSTEM}] + history + [{\"role\": \"user\", \"content\": query}]\n",
        "        out = client.chat_completion(model=ROUTER_MODEL, messages=messages, max_tokens=64, temperature=0.1)\n",
        "        text = out.choices[0].message[\"content\"] if hasattr(out.choices[0], \"message\") else out.choices[0].message.content\n",
        "        route = json.loads(text).get(\"route\", \"general\")\n",
        "        return route\n",
        "    except Exception as e:\n",
        "        # Heuristic routing\n",
        "        q = query.lower()\n",
        "        if any(k in q for k in [\"generate image\", \"make a picture\", \"draw\", \"sdxl\", \"stable diffusion\", \"image of\"]):\n",
        "            return \"image_generation\"\n",
        "        if any(k in q for k in [\"tts\", \"text to speech\", \"voice\", \"say this\", \"audio generate\"]):\n",
        "            return \"audio_tts\"\n",
        "        if any(k in q for k in [\"transcribe\", \"asr\", \"speech to text\", \"what is being said\"]):\n",
        "            return \"asr_transcribe\"\n",
        "        if any(k in q for k in [\"write code\", \"implement\", \"function\", \"script\", \"class\", \"boilerplate\"]):\n",
        "            return \"code_generation\"\n",
        "        if any(k in q for k in [\"fix\", \"bug\", \"optimize\", \"refactor\", \"review my code\", \"lint\"]):\n",
        "            return \"code_review\"\n",
        "        if any(k in q for k in [\"search\", \"find sources\", \"web\", \"google\", \"duckduckgo\"]):\n",
        "            return \"web_search\"\n",
        "        if any(k in q for k in [\"fetch\", \"download page\", \"scrape\", \"clean this url\", \"extract from url\", \"http://\", \"https://\"]):\n",
        "            return \"web_fetch\"\n",
        "        if any(k in q for k in [\"extract table\", \"convert to csv\", \"make dataset\", \"tabular\"]):\n",
        "            return \"extract_table\"\n",
        "        return \"general\"\n",
        "\n",
        "print(\"Router ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Model map (switch per task)\n",
        "\n",
        "Below is a **default routing table**. Swap any model IDs for your preferences/endpoints.\n",
        "\n",
        "- Code gen / review: `Qwen2.5-Coder`, `DeepSeek-Coder`, `HuggingFaceH4/zephyr-7b-beta`, etc.\n",
        "- Image gen: SDXL / Flux family (requires GPU endpoint).\n",
        "- Audio: a TTS checkpoint and an ASR (Whisper) model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "MODEL_MAP = {\n",
        "    # text/code\n",
        "    \"code_generation\": \"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
        "    \"code_review\": \"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    \"general\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    # image\n",
        "    \"image_generation\": \"stabilityai/stable-diffusion-xl-base-1.0\",  # or black-forest-labs/FLUX.1-schnell\n",
        "    # audio\n",
        "    \"audio_tts\": \"espnet/kan-bayashi-ljspeech-vits\",\n",
        "    \"asr_transcribe\": \"openai/whisper-large-v3\",  # choose a public HF ASR endpoint you can access\n",
        "}\n",
        "\n",
        "def pick_model(route: str) -> str:\n",
        "    return MODEL_MAP.get(route, MODEL_MAP[\"general\"]) \n",
        "\n",
        "print(MODEL_MAP)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) HF Inference helpers\n",
        "\n",
        "Small wrappers for **chat**, **image**, and **audio** calls using the Hugging Face Inference API.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def chat_llm(messages: List[Dict[str, str]], model: Optional[str] = None, **kw) -> str:\n",
        "    model = model or MODEL_MAP[\"general\"]\n",
        "    if not HF_TOKEN:\n",
        "        return \"[DRY-RUN] (No HF token) Would call chat_completion on: \" + model\n",
        "    client = hf_client()\n",
        "    out = client.chat_completion(model=model, messages=messages, temperature=kw.get(\"temperature\", 0.2), max_tokens=kw.get(\"max_tokens\", 1024))\n",
        "    return out.choices[0].message[\"content\"] if hasattr(out.choices[0], \"message\") else out.choices[0].message.content\n",
        "\n",
        "def text_to_image(prompt: str, model: Optional[str] = None, out_name: str = \"image.png\") -> str:\n",
        "    model = model or MODEL_MAP[\"image_generation\"]\n",
        "    if not HF_TOKEN:\n",
        "        return \"[DRY-RUN] (No HF token) Would generate image with: \" + model\n",
        "    url = f\"https://api-inference.huggingface.co/models/{model}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\", \"Accept\": \"image/png\"}\n",
        "    payload = {\"inputs\": prompt}\n",
        "    r = httpx.post(url, headers=headers, json=payload, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    out_path = ARTIFACTS_DIR / out_name\n",
        "    out_path.write_bytes(r.content)\n",
        "    return str(out_path)\n",
        "\n",
        "def tts(text: str, model: Optional[str] = None, out_name: str = \"speech.wav\") -> str:\n",
        "    model = model or MODEL_MAP[\"audio_tts\"]\n",
        "    if not HF_TOKEN:\n",
        "        return \"[DRY-RUN] (No HF token) Would synthesize TTS with: \" + model\n",
        "    client = hf_client()\n",
        "    audio_bytes = client.text_to_speech(text=text, model=model)\n",
        "    out_path = ARTIFACTS_DIR / out_name\n",
        "    out_path.write_bytes(audio_bytes)\n",
        "    return str(out_path)\n",
        "\n",
        "def asr(audio_path: str, model: Optional[str] = None) -> str:\n",
        "    model = model or MODEL_MAP[\"asr_transcribe\"]\n",
        "    if not HF_TOKEN:\n",
        "        return \"[DRY-RUN] (No HF token) Would transcribe with: \" + model\n",
        "    client = hf_client()\n",
        "    with open(audio_path, \"rb\") as f:\n",
        "        result = client.automatic_speech_recognition(audio=f, model=model)\n",
        "    return result.get(\"text\") if isinstance(result, dict) else str(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Agents\n",
        "\n",
        "We’ll define three simple agents and wire them to the router:\n",
        "\n",
        "- **CodeAgent**: draft → *reflection* critique → revision (optionally run tiny tests)\n",
        "- **MediaAgent**: image/audio generation with iterative refinement\n",
        "- **WebAgent**: search → fetch → extract → structure (CSV/JSON)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def code_generate(prompt: str, language: str = \"python\") -> str:\n",
        "    model = pick_model(\"code_generation\")\n",
        "    system = f\"You write high-quality, idiomatic {language} code with docstrings and simple tests when asked.\"\n",
        "    return chat_llm(\n",
        "        [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": prompt}],\n",
        "        model=model\n",
        "    )\n",
        "\n",
        "def code_reflect(code_text: str, language: str = \"python\") -> str:\n",
        "    model = pick_model(\"code_review\")\n",
        "    system = f\"You are a strict {language} code reviewer. Find bugs, edge cases, complexity issues. Propose concrete, minimal diffs.\"\n",
        "    return chat_llm(\n",
        "        [{\"role\": \"system\", \"content\": system},\n",
        "         {\"role\": \"user\", \"content\": f\"Review this code and propose improved version with reasons.\\n\\n```{language}\\n{code_text}\\n```\"}],\n",
        "        model=model\n",
        "    )\n",
        "\n",
        "def code_revision(prompt: str, draft: str, review: str, language: str = \"python\") -> str:\n",
        "    model = pick_model(\"code_generation\")\n",
        "    system = f\"Revise the code based on the review. Keep it concise, correct, and tested. Language: {language}.\"\n",
        "    return chat_llm(\n",
        "        [{\"role\": \"system\", \"content\": system},\n",
        "         {\"role\": \"user\", \"content\": f\"User request: {prompt}\\n\\nDraft code:\\n```{language}\\n{draft}\\n```\\n\\nReview feedback:\\n{review}\\n\\nPlease output only the final improved code in a single fenced block.\"}],\n",
        "        model=model\n",
        "    )\n",
        "\n",
        "def run_codegen_reflection(user_prompt: str, language: str = \"python\") -> Dict[str, str]:\n",
        "    route = route_with_model(user_prompt)\n",
        "    if route not in (\"code_generation\", \"code_review\", \"general\"):\n",
        "        print(f\"Router suggested {route}; overriding to code_generation for this workflow.\")\n",
        "    draft = code_generate(user_prompt, language)\n",
        "    review = code_reflect(draft, language)\n",
        "    final = code_revision(user_prompt, draft, review, language)\n",
        "    return {\"route\": route, \"draft\": draft, \"review\": review, \"final\": final}\n",
        "\n",
        "def media_generate_and_refine(kind: str, prompt: str, feedback: Optional[str] = None) -> Dict[str, str]:\n",
        "    assert kind in (\"image\", \"audio\"), \"kind must be 'image' or 'audio'\"\n",
        "    if kind == \"image\":\n",
        "        route = route_with_model(\"generate image: \" + prompt)\n",
        "        path1 = text_to_image(prompt, out_name=\"image_v1.png\")\n",
        "        if isinstance(path1, str) and path1.startswith(\"[DRY-RUN]\"):\n",
        "            refined_path = path1\n",
        "        else:\n",
        "            fb = feedback or chat_llm(\n",
        "                [{\"role\": \"system\", \"content\": \"You improve prompts for image generation.\"},\n",
        "                 {\"role\": \"user\", \"content\": f\"Original prompt: {prompt}. Suggest a refined prompt to improve composition/lighting/details.\"}]\n",
        "            )\n",
        "            refined_prompt = f\"{prompt}. Refinements: {fb}\"\n",
        "            refined_path = text_to_image(refined_prompt, out_name=\"image_v2.png\")\n",
        "        return {\"route\": route, \"v1\": str(path1), \"v2\": str(refined_path)}\n",
        "    else:\n",
        "        route = route_with_model(\"tts: \" + prompt)\n",
        "        p1 = tts(prompt, out_name=\"speech_v1.wav\")\n",
        "        if isinstance(p1, str) and p1.startswith(\"[DRY-RUN]\"):\n",
        "            p2 = p1\n",
        "        else:\n",
        "            fb = feedback or chat_llm(\n",
        "                [{\"role\": \"system\", \"content\": \"You improve TTS prompts: pacing, emphasis, style.\"},\n",
        "                 {\"role\": \"user\", \"content\": f\"Original text: {prompt}. Suggest SSML-like cues to improve prosody.\"}]\n",
        "            )\n",
        "            refined_text = f\"{prompt}\\n\\nCues: {fb}\"\n",
        "            p2 = tts(refined_text, out_name=\"speech_v2.wav\")\n",
        "        return {\"route\": route, \"v1\": str(p1), \"v2\": str(p2)}\n",
        "\n",
        "def web_search(query: str, max_results: int = 5) -> List[Tuple[str, str]]:\n",
        "    try:\n",
        "        from duckduckgo_search import DDGS\n",
        "    except Exception:\n",
        "        print(\"Install duckduckgo-search to enable web_search.\")\n",
        "        return []\n",
        "    results = []\n",
        "    with DDGS() as ddgs:\n",
        "        for r in ddgs.text(query, max_results=max_results):\n",
        "            results.append((r.get(\"title\", \"\"), r.get(\"href\", \"\")))\n",
        "    return results\n",
        "\n",
        "def web_fetch(url: str) -> Dict[str, str]:\n",
        "    import bs4, readability\n",
        "    r = httpx.get(url, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    doc = readability.Document(r.text)\n",
        "    title = doc.short_title()\n",
        "    html = doc.summary()\n",
        "    soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
        "    text = soup.get_text(\" \")\n",
        "    return {\"title\": title or \"\", \"text\": text}\n",
        "\n",
        "def extract_table(text: str, out_csv: str = \"extracted.csv\") -> str:\n",
        "    import pandas as pd\n",
        "    system = \"You are a data wrangler. Given raw webpage text, extract a small table (up to ~20 rows) in CSV format with headers.\"\n",
        "    csv_text = chat_llm(\n",
        "        [{\"role\": \"system\", \"content\": system},\n",
        "         {\"role\": \"user\", \"content\": text[:12000]}],\n",
        "        max_tokens=1200\n",
        "    )\n",
        "    try:\n",
        "        from io import StringIO\n",
        "        df = pd.read_csv(StringIO(csv_text))\n",
        "    except Exception:\n",
        "        df = pd.DataFrame({\"extracted\": [csv_text]})\n",
        "    out_path = ARTIFACTS_DIR / out_csv\n",
        "    df.to_csv(out_path, index=False)\n",
        "    return str(out_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick demos (run locally)\n",
        "\n",
        "These calls exercise the agents directly (without MCP)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 1) Code gen + reflection\n",
        "res = run_codegen_reflection(\"Write a Python function that returns the nth Fibonacci number iteratively.\")\n",
        "print(\"ROUTE:\", res[\"route\"])\n",
        "print(\"\\n--- DRAFT ---\\n\", res[\"draft\"][:400], \"...\\n\")\n",
        "print(\"\\n--- REVIEW ---\\n\", res[\"review\"][:400], \"...\\n\")\n",
        "print(\"\\n--- FINAL ---\\n\", res[\"final\"][:400], \"...\\n\")\n",
        "\n",
        "# 2) Image gen + refinement (will save to artifacts/)\n",
        "img = media_generate_and_refine(\"image\", \"a cozy study room at golden hour, volumetric light, photorealistic\")\n",
        "print(img)\n",
        "\n",
        "# 3) Audio TTS + refinement (will save to artifacts/)\n",
        "aud = media_generate_and_refine(\"audio\", \"Welcome to the Omni + MCP tutorial! This line will be synthesized.\")\n",
        "print(aud)\n",
        "\n",
        "# 4) Web scraping flow\n",
        "hits = web_search(\"latest advances in battery technology site:arxiv.org\", max_results=3)\n",
        "print(hits)\n",
        "if hits:\n",
        "    page = web_fetch(hits[0][1])\n",
        "    csv_path = extract_table(page[\"text\"])  # heuristic LLM extraction\n",
        "    print(\"Saved:\", csv_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) MCP Server (tools for agentic workflows)\n",
        "\n",
        "We’ll now write a minimal MCP server script to `artifacts/omni_mcp_server.py`. It exposes tools:\n",
        "- `infer_intent` (router)\n",
        "- `code_generate`, `code_review`\n",
        "- `image_generate`\n",
        "- `audio_tts`, `asr_transcribe`\n",
        "- `web_search`, `web_fetch`, `extract_table`\n",
        "\n",
        "**Run it:**\n",
        "```bash\n",
        "python artifacts/omni_mcp_server.py\n",
        "```\n",
        "Then register this server in your MCP client (Claude Desktop / Cursor / VS Code extension).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "mcp_server_code = \"\"\"# omni_mcp_server.py\n",
        "import os, json, httpx, base64, tempfile, requests\n",
        "from typing import List, Dict, Any\n",
        "from mcp.server import Server\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\", \"\")\n",
        "ROUTER_MODEL = os.getenv(\"ROUTER_MODEL\", \"katanemo/Arch-Router-1.5B\")\n",
        "\n",
        "MODEL_MAP = {\n",
        "    \"code_generation\": os.getenv(\"MODEL_CODEGEN\", \"Qwen/Qwen2.5-Coder-7B-Instruct\"),\n",
        "    \"code_review\":    os.getenv(\"MODEL_CODEREVIEW\", \"HuggingFaceH4/zephyr-7b-beta\"),\n",
        "    \"general\":        os.getenv(\"MODEL_GENERAL\", \"mistralai/Mistral-7B-Instruct-v0.3\"),\n",
        "    \"image_generation\": os.getenv(\"MODEL_IMAGE\", \"stabilityai/stable-diffusion-xl-base-1.0\"),\n",
        "    \"audio_tts\":        os.getenv(\"MODEL_TTS\", \"espnet/kan-bayashi-ljspeech-vits\"),\n",
        "    \"asr_transcribe\":   os.getenv(\"MODEL_ASR\", \"openai/whisper-large-v3\"),\n",
        "}\n",
        "\n",
        "def hf_client():\n",
        "    return InferenceClient(token=HF_TOKEN)\n",
        "\n",
        "def chat_llm(messages: List[Dict[str, str]], model: str) -> str:\n",
        "    if not HF_TOKEN:\n",
        "        return \"[DRY-RUN] Missing HF_TOKEN.\"\n",
        "    out = hf_client().chat_completion(model=model, messages=messages, temperature=0.2, max_tokens=1024)\n",
        "    return out.choices[0].message[\"content\"] if hasattr(out.choices[0], \"message\") else out.choices[0].message.content\n",
        "\n",
        "def route(query: str) -> str:\n",
        "    if not HF_TOKEN:\n",
        "        q = query.lower()\n",
        "        if any(k in q for k in [\"image\", \"draw\", \"picture\"]): return \"image_generation\"\n",
        "        if any(k in q for k in [\"tts\", \"voice\"]): return \"audio_tts\"\n",
        "        if any(k in q for k in [\"transcribe\", \"asr\"]): return \"asr_transcribe\"\n",
        "        if any(k in q for k in [\"function\", \"script\", \"class\", \"code\"]): return \"code_generation\"\n",
        "        if any(k in q for k in [\"review\", \"refactor\", \"bugs\"]): return \"code_review\"\n",
        "        if any(k in q for k in [\"search\", \"google\", \"find\"]): return \"web_search\"\n",
        "        if any(k in q for k in [\"http://\", \"https://\", \"fetch\", \"scrape\"]): return \"web_fetch\"\n",
        "        return \"general\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a router; respond JSON: {\\\"route\\\": \\\"<name>\\\"}.\"},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "    try:\n",
        "        text = chat_llm(messages, ROUTER_MODEL)\n",
        "        return json.loads(text).get(\"route\", \"general\")\n",
        "    except Exception:\n",
        "        return \"general\"\n",
        "\n",
        "server = Server(\"omni-mcp\")\n",
        "\n",
        "@server.tool()\n",
        "def infer_intent(query: str) -> str:\n",
        "    \"\"\"Return a route name for the user's query.\"\"\"\n",
        "    return route(query)\n",
        "\n",
        "@server.tool()\n",
        "def code_generate(prompt: str, language: str = \"python\") -> str:\n",
        "    model = MODEL_MAP[\"code_generation\"]\n",
        "    sys = f\"Write high-quality {language} code with docstrings.\"\n",
        "    return chat_llm([{\"role\": \"system\", \"content\": sys}, {\"role\": \"user\", \"content\": prompt}], model)\n",
        "\n",
        "@server.tool()\n",
        "def code_review(code: str, language: str = \"python\") -> str:\n",
        "    model = MODEL_MAP[\"code_review\"]\n",
        "    sys = f\"Review {language} code; find bugs & propose improved version with reasons.\"\n",
        "    return chat_llm([{\"role\": \"system\", \"content\": sys}, {\"role\": \"user\", \"content\": code}], model)\n",
        "\n",
        "@server.tool()\n",
        "def image_generate(prompt: str) -> str:\n",
        "    model = MODEL_MAP[\"image_generation\"]\n",
        "    if not HF_TOKEN:\n",
        "        return \"[DRY-RUN] Missing HF_TOKEN.\"\n",
        "    url = f\"https://api-inference.huggingface.co/models/{model}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\", \"Accept\": \"image/png\"}\n",
        "    r = httpx.post(url, headers=headers, json={\"inputs\": prompt}, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    b64 = base64.b64encode(r.content).decode(\"utf-8\")\n",
        "    return json.dumps({\"image_base64\": b64})\n",
        "\n",
        "@server.tool()\n",
        "def audio_tts(text: str) -> str:\n",
        "    model = MODEL_MAP[\"audio_tts\"]\n",
        "    if not HF_TOKEN:\n",
        "        return \"[DRY-RUN] Missing HF_TOKEN.\"\n",
        "    audio_bytes = InferenceClient(token=HF_TOKEN).text_to_speech(text=text, model=model)\n",
        "    return f\"AUDIO_BYTES:{base64.b64encode(audio_bytes).decode('utf-8')}\"\n",
        "\n",
        "@server.tool()\n",
        "def asr_transcribe(path_or_url: str) -> str:\n",
        "    model = MODEL_MAP[\"asr_transcribe\"]\n",
        "    if not HF_TOKEN:\n",
        "        return \"[DRY-RUN] Missing HF_TOKEN.\"\n",
        "    client = InferenceClient(token=HF_TOKEN)\n",
        "    if path_or_url.startswith(\"http\"):\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp:\n",
        "            tmp.write(requests.get(path_or_url).content)\n",
        "            audio_path = tmp.name\n",
        "    else:\n",
        "        audio_path = path_or_url\n",
        "    with open(audio_path, 'rb') as f:\n",
        "        result = client.automatic_speech_recognition(audio=f, model=model)\n",
        "    return result.get(\"text\") if isinstance(result, dict) else str(result)\n",
        "\n",
        "@server.tool()\n",
        "def web_search(query: str, max_results: int = 5) -> str:\n",
        "    try:\n",
        "        from duckduckgo_search import DDGS\n",
        "        hits = []\n",
        "        with DDGS() as ddgs:\n",
        "            for r in ddgs.text(query, max_results=max_results):\n",
        "                hits.append({\"title\": r.get(\"title\", \"\"), \"url\": r.get(\"href\", \"\")})\n",
        "        return json.dumps(hits)\n",
        "    except Exception as e:\n",
        "        return json.dumps({\"error\": str(e)})\n",
        "\n",
        "@server.tool()\n",
        "def web_fetch(url: str) -> str:\n",
        "    import bs4, readability\n",
        "    r = httpx.get(url, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    doc = readability.Document(r.text)\n",
        "    title = doc.short_title()\n",
        "    html = doc.summary()\n",
        "    soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
        "    text = soup.get_text(\" \")\n",
        "    return json.dumps({\"title\": title or \"\", \"text\": text})\n",
        "\n",
        "@server.tool()\n",
        "def extract_table(text: str) -> str:\n",
        "    messages = [{\"role\": \"system\", \"content\": \"Return CSV only.\"}, {\"role\": \"user\", \"content\": text[:12000]}]\n",
        "    csv_text = chat_llm(messages, MODEL_MAP[\"general\"])\n",
        "    return csv_text\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    server.run_stdio()\n",
        "\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "ARTIFACTS_DIR = Path(\"artifacts\"); ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
        "(ARTIFACTS_DIR / \"omni_mcp_server.py\").write_text(mcp_server_code, encoding=\"utf-8\")\n",
        "print(\"Wrote:\", (ARTIFACTS_DIR / \"omni_mcp_server.py\").resolve())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Putting it together — three **agentic** workflows\n",
        "\n",
        "Below are thin, user‑facing wrappers that (a) **infer intent** with the router and (b) hand off to the right agent. You can call them directly or surface them through MCP tools.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def omni_handle(user_request: str, extra: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
        "    extra = extra or {}\n",
        "    route = route_with_model(user_request)\n",
        "    model = pick_model(route)\n",
        "    out: Dict[str, Any] = {\"route\": route, \"model\": model}\n",
        "    if route == \"code_generation\":\n",
        "        out.update(run_codegen_reflection(user_request))\n",
        "    elif route == \"code_review\":\n",
        "        out[\"review\"] = code_reflect(extra.get(\"code\", user_request))\n",
        "    elif route == \"image_generation\":\n",
        "        out.update(media_generate_and_refine(\"image\", user_request))\n",
        "    elif route == \"audio_tts\":\n",
        "        out.update(media_generate_and_refine(\"audio\", user_request))\n",
        "    elif route == \"asr_transcribe\":\n",
        "        audio_path = extra.get(\"audio_path\", \"\")\n",
        "        out[\"transcript\"] = asr(audio_path) if audio_path else \"No audio_path provided.\"\n",
        "    elif route == \"web_search\":\n",
        "        out[\"results\"] = web_search(user_request)\n",
        "    elif route == \"web_fetch\":\n",
        "        url = extra.get(\"url\") or user_request\n",
        "        page = web_fetch(url)\n",
        "        out.update(page)\n",
        "        out[\"csv_path\"] = extract_table(page.get(\"text\", \"\"))\n",
        "    else:\n",
        "        out[\"answer\"] = chat_llm([{\"role\": \"user\", \"content\": user_request}])\n",
        "    return out\n",
        "\n",
        "print(omni_handle(\"Generate an image of a hummingbird in flight at sunrise\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Configuration tips & safety\n",
        "\n",
        "- Set `HF_TOKEN` (and optionally override models with `MODEL_*` env vars) for real calls.\n",
        "- Respect website **robots** and terms when scraping. Keep request volume low.\n",
        "- **Images/Audio** via public models may contain artifacts; use iterative refinement.\n",
        "- **Code exec** is *not* performed here by default; if you add one, sandbox with time/memory limits.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Notebook generated on: 2025-10-22 03:41:32*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}